# Perceptron e Adaline

<p align="justify">Bem-vindo ao reposit√≥rio dos algoritmos Perceptron e Adaline, dois conceitos fundamentais no campo do aprendizado de m√°quina e das redes neurais artificiais. Aprender e dominar esses algoritmos √© crucial para compreender conceitos mais avan√ßados e aplicar a intelig√™ncia artificial em uma ampla gama de tarefas.</p>
<p align="center">

<p  align="center">
<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif">             
<br>
</p>

<p align="center">
<img src="https://miro.medium.com/v2/resize:fit:640/1*ZS7xxm9jkGIcRnH3QKs02g.gif"/>
</p>

# :bookmark_tabs: Algoritmos


### :snake: Revis√£o Python 

> <p align="justify"> A revis√£o de Python √© uma introdu√ß√£o b√°sica √† linguagem de programa√ß√£o Python, uma linguagem amplamente utilizada em ci√™ncia de dados e aprendizado de m√°quina. Essa se√ß√£o aborda os fundamentos da linguagem, incluindo sintaxe, tipos de dados, estruturas de controle e fun√ß√µes. Essa revis√£o √© essencial para quem est√° come√ßando a aprender sobre redes neurais e algoritmos de aprendizado de m√°quina, pois oferece uma base s√≥lida para a compreens√£o e implementa√ß√£o dos algoritmos Perceptron e Adaline.</p>

 # :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Revisao.ipynb) 

### :panda_face: Revis√£o Pandas e Seaborn - Titanic
> <p align="justify"> A revis√£o de Pandas e Seaborn aborda duas bibliotecas fundamentais para a an√°lise e visualiza√ß√£o de dados em Python. Pandas √© uma biblioteca de manipula√ß√£o de dados que fornece estruturas de dados flex√≠veis, como DataFrames, para trabalhar com dados tabulares. Seaborn √© uma biblioteca de visualiza√ß√£o de dados baseada no Matplotlib que facilita a cria√ß√£o de gr√°ficos estat√≠sticos informativos e atraentes.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Titanic.ipynb) 

### :chart_with_upwards_trend: Revis√£o de Vetores e Encadeamento 
> <p align="justify"> A revis√£o de Vetores e Encadeamento de C√≥digo aborda conceitos importantes relacionados √† manipula√ß√£o de vetores e √† organiza√ß√£o eficiente de c√≥digo em Python. Vetores s√£o estruturas de dados unidimensionais que armazenam uma cole√ß√£o de elementos do mesmo tipo, como n√∫meros, e s√£o comumente usados em aprendizado de m√°quina e redes neurais artificiais para representar dados e realizar c√°lculos.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Vetores_e_Encadeamento.ipynb) 

### üß† Perceptron uma Camada 
><p align="justify"> O Perceptron de uma Camada √© um dos algoritmos fundamentais no campo do aprendizado de m√°quina e das redes neurais artificiais. Trata-se de um modelo linear simples baseado no neur√¥nio artificial, que recebe v√°rias entradas e produz uma sa√≠da √∫nica. O Perceptron de uma Camada √© usado principalmente para tarefas de classifica√ß√£o linear, ou seja, para separar dados em duas classes distintas com base em um hiperplano.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Perceptron_Monolayer.ipynb) 


### üß† Perceptron para Regress√£o Linear 
> <p align="justify"> O algoritmo aprende a partir de um conjunto de dados de treinamento, ajustando os pesos associados √†s entradas e o limiar de decis√£o (bias) de forma iterativa. O processo de aprendizado envolve a atualiza√ß√£o dos pesos e do limiar de acordo com a regra de aprendizado do Perceptron, que minimiza os erros de regress√£o linear.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Perceptron_para_regress%C3%A3o_linear.ipynb) 


### üß† Perceptron uma Camada com Numpy
><p align="justify"> Ao utilizar a biblioteca NumPy, √© poss√≠vel melhorar a efici√™ncia e a simplicidade do c√≥digo Perceptron de uma Camada, aproveitando as fun√ß√µes e opera√ß√µes vetoriais fornecidas pela biblioteca. Isso permite que voc√™ se concentre nos conceitos e na l√≥gica do algoritmo, enquanto o NumPy cuida dos detalhes relacionados √† manipula√ß√£o de vetores e matrizes.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Perceptron_Numpy.ipynb) 


### :o: TLU 
> <p align="justify"> A TLU (Threshold Logic Unit) √© um tipo de neur√¥nio artificial que serve como base para o Perceptron de uma Camada e outros algoritmos de aprendizado de m√°quina. A TLU recebe v√°rias entradas, aplica pesos a essas entradas, soma os produtos e, em seguida, passa o resultado por uma fun√ß√£o de ativa√ß√£o limiar. Essa fun√ß√£o de ativa√ß√£o produz uma sa√≠da √∫nica, normalmente representando uma classe ou categoria em problemas de classifica√ß√£o.</p>
# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/TLU.ipynbb) 

### üß† Adaline 
> <p align="justify"> Adaline √© um acr√¥nimo para Adaptive Linear Neuron, que √© um tipo de neur√¥nio artificial desenvolvido por Bernard Widrow e Marcian Hoff em 1960. Adaline √© uma extens√£o do Perceptron de uma Camada e utiliza o mesmo conceito de neur√¥nio artificial com entradas ponderadas e uma fun√ß√£o de ativa√ß√£o. No entanto, a diferen√ßa fundamental entre o Perceptron e o Adaline est√° na forma como os pesos s√£o atualizados durante o treinamento.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Pr%C3%A1tica_Adaline.ipynb)

### :chart_with_downwards_trend: Derivada
> <p align="justify"> A derivada √© um conceito fundamental no c√°lculo diferencial e tem aplica√ß√µes importantes em v√°rias √°reas, incluindo aprendizado de m√°quina, otimiza√ß√£o e an√°lise de fun√ß√µes. Em termos simples, a derivada de uma fun√ß√£o representa a taxa de varia√ß√£o da fun√ß√£o em rela√ß√£o a uma de suas vari√°veis. A derivada nos fornece informa√ß√µes sobre a inclina√ß√£o de uma fun√ß√£o em um ponto espec√≠fico, o que pode ser √∫til para determinar extremos locais (m√°ximos e m√≠nimos) e pontos de inflex√£o.</p>
# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Derivada.ipynb)

### :twisted_rightwards_arrows: Conceito de Gradiente
><p align="justify"> O gradiente √© um conceito central em otimiza√ß√£o e aprendizado de m√°quina. Ele representa um vetor que cont√©m as derivadas parciais de uma fun√ß√£o multivari√°vel em rela√ß√£o a cada uma de suas vari√°veis. O gradiente fornece informa√ß√µes sobre a dire√ß√£o de maior aumento na fun√ß√£o, ou seja, a dire√ß√£o na qual a fun√ß√£o cresce mais rapidamente. A magnitude do gradiente indica a taxa de aumento da fun√ß√£o nessa dire√ß√£o.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Gradiente.ipynb)

### :arrow_double_down: Gradiente Descendente
> <p align="justify"> O Gradiente Descendente √© um algoritmo de otimiza√ß√£o amplamente utilizado em aprendizado de m√°quina e redes neurais. Sua principal fun√ß√£o √© minimizar uma fun√ß√£o de custo (ou fun√ß√£o de erro) ajustando os par√¢metros do modelo, como pesos e vi√©s, de forma iterativa. O algoritmo utiliza o gradiente da fun√ß√£o de custo para determinar a dire√ß√£o de maior decr√©scimo, ou seja, a dire√ß√£o na qual a fun√ß√£o de custo diminui mais rapidamente.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Gradiente_Descendente.ipynb)

### :eight_pointed_black_star: Rede Neural + Gradiente Descendente

> <p align="justify"> Combinar redes neurais com o algoritmo de otimiza√ß√£o Gradiente Descendente √© uma abordagem poderosa para treinar modelos de aprendizado de m√°quina capazes de aprender e se adaptar aos dados de treinamento. O Gradiente Descendente √© usado para ajustar os pesos e vi√©s dos neur√¥nios nas camadas da rede neural de forma a minimizar a fun√ß√£o de custo. O processo de treinamento consiste em passar os dados de treinamento pela rede, calcular o erro (ou custo) e, em seguida, usar o gradiente desse erro para atualizar os par√¢metros do modelo. </p>


# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Rede_Neural_com_GD_.ipynb)

### :globe_with_meridians: MultiLayer Perceptron Simples
> <p align="justify"> O MultiLayer Perceptron (MLP) √© uma arquitetura de rede neural artificial que consiste em m√∫ltiplas camadas de neur√¥nios. Esta arquitetura √© mais avan√ßada e capaz de lidar com problemas mais complexos do que o Perceptron de uma camada. Um MLP simples geralmente possui uma camada de entrada, uma ou mais camadas ocultas e uma camada de sa√≠da.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/MLP_Simples.ipynb)

### :rewind: Backpropagation
> <p align="justify">  O algoritmo de Backpropagation, tamb√©m conhecido como retropropaga√ß√£o, √© uma t√©cnica fundamental no treinamento de redes neurais artificiais, especialmente em arquiteturas MultiLayer Perceptron (MLP). O objetivo do Backpropagation √© calcular o gradiente da fun√ß√£o de custo em rela√ß√£o aos par√¢metros (pesos e vi√©s) da rede neural, de modo a permitir que o algoritmo de Gradiente Descendente ajuste os par√¢metros e minimize a fun√ß√£o de custo.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Aplica%C3%A7%C3%A3o_Backpropagation.ipynb)

### :part_alternation_mark: A√ß√µes da Americanas (AMER3)
> <p align="justify"> A aplica√ß√£o de redes neurais artificiais no mercado financeiro, como a bolsa de valores, tem ganhado popularidade devido √† sua capacidade de aprender padr√µes complexos e realizar previs√µes com base em dados hist√≥ricos. Investidores e analistas financeiros t√™m explorado o uso de redes neurais para modelar e prever o comportamento dos pre√ßos das a√ß√µes, √≠ndices e outros instrumentos financeiros, com o objetivo de tomar decis√µes de investimento mais informadas e melhorar a rentabilidade.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/AMERICANAS.ipynb)

### :warning: MLP para Detectar Malware
> <p align="justify"> As redes neurais podem ser treinadas para identificar padr√µes e caracter√≠sticas espec√≠ficas de arquivos e processos maliciosos. Utilizando t√©cnicas de aprendizado profundo e conjuntos de dados rotulados contendo exemplos de malwares e arquivos benignos, √© poss√≠vel criar modelos de classifica√ß√£o capazes de distinguir entre software malicioso e leg√≠timo com alta precis√£o.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Classifica%C3%A7%C3%A3o.ipynb)

### :no_entry: Resolvendo a Porta XOR
><p align="justify">  A porta XOR (exclusive OR) √© uma fun√ß√£o l√≥gica bin√°ria que retorna verdadeiro (1) apenas quando o n√∫mero de entradas verdadeiras (1s) √© √≠mpar. Em outras palavras, ela produz 1 quando as entradas s√£o diferentes e 0 quando as entradas s√£o iguais. O problema da porta XOR √© um exemplo cl√°ssico em aprendizado de m√°quina e redes neurais, pois representa um problema de classifica√ß√£o n√£o linearmente separ√°vel que n√£o pode ser resolvido diretamente por um √∫nico perceptron de camada √∫nica.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Problema_XOR.ipynb)

### :bar_chart: Regress√£o Linear
><p align="justify"> A regress√£o linear √© um m√©todo estat√≠stico que busca modelar a rela√ß√£o entre uma vari√°vel dependente e uma ou mais vari√°veis independentes. Embora a regress√£o linear possa ser eficientemente resolvida usando m√©todos matem√°ticos tradicionais, como o m√©todo dos m√≠nimos quadrados, tamb√©m √© poss√≠vel abord√°-la usando redes neurais artificiais. Nesse caso, a rede neural √© configurada para ter uma √∫nica camada de entrada com tantos neur√¥nios quanto as vari√°veis independentes, e uma √∫nica camada de sa√≠da com um √∫nico neur√¥nio que representa a vari√°vel dependente.Ao utilizar uma fun√ß√£o de ativa√ß√£o linear na camada de sa√≠da e treinar a rede neural com algoritmos como o gradiente descendente ou o backpropagation, a rede neural aprender√° os coeficientes da equa√ß√£o de regress√£o linear que melhor se ajusta aos dados. Dessa forma, a rede neural se torna um modelo de regress√£o linear.</p>

# :computer: [![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/Rafael-Barbosa/Perceptron_Adaline/blob/main/Simples_Regress%C3%A3o.ipynb)



